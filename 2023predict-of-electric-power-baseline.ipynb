{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data manipulation\n",
        "includes importing the data and feature engineering"
      ],
      "metadata": {
        "id": "82Ta13bpPaWe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-10-03T14:30:04.545342Z",
          "iopub.status.busy": "2023-10-03T14:30:04.544551Z",
          "iopub.status.idle": "2023-10-03T14:30:04.550390Z",
          "shell.execute_reply": "2023-10-03T14:30:04.549132Z",
          "shell.execute_reply.started": "2023-10-03T14:30:04.545302Z"
        },
        "id": "us9UYG1E1MZv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Library for importing csv files\n",
        "import pandas as pd\n",
        "#Library for matrix operations\n",
        "import numpy as np\n",
        "#Set random seeds to ensure that the model can be reproduced\n",
        "import random\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## importing training data and test data\n",
        "\n",
        "- important note: the the test dataframe refers to the dataset which we want to predict the values and measure the RMSE, its not involved in the training and test phase. It is used after the traing is done."
      ],
      "metadata": {
        "id": "q1NofqdGQGC5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-10-03T14:30:04.556580Z",
          "iopub.status.busy": "2023-10-03T14:30:04.555843Z",
          "iopub.status.idle": "2023-10-03T14:30:05.017935Z",
          "shell.execute_reply": "2023-10-03T14:30:05.016306Z",
          "shell.execute_reply.started": "2023-10-03T14:30:04.556545Z"
        },
        "id": "olNTK4Ep1MZx",
        "outputId": "2b08c352-f76a-40a6-db91-aba855f807e1",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_df):329304\n",
            "len(test_df):17136\n"
          ]
        }
      ],
      "source": [
        "# importing data which is in .csv format\n",
        "train_df=pd.read_csv(\"train_df.csv\")\n",
        "print(f\"len(train_df):{len(train_df)}\")\n",
        "test_df=pd.read_csv(\"test_df.csv\")\n",
        "print(f\"len(test_df):{len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concatinating the training and test data frames for futher data manipulation\n",
        "total_df=pd.concat((train_df,test_df),axis=0)\n",
        "# dropping f1 and f2 columns in the dataset since it is not informative\n",
        "total_df.drop([\"f1\",\"f2\"], axis=1,inplace= True)\n",
        "print(total_df.info())"
      ],
      "metadata": {
        "id": "S4VAcBpKQrie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07f91d5-55a3-4330-cbe0-0c3b130699be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 346440 entries, 0 to 17135\n",
            "Data columns (total 8 columns):\n",
            " #   Column        Non-Null Count   Dtype  \n",
            "---  ------        --------------   -----  \n",
            " 0   id            346440 non-null  int64  \n",
            " 1   id_encode     346440 non-null  int64  \n",
            " 2   hour          346440 non-null  int64  \n",
            " 3   parking_free  346440 non-null  int64  \n",
            " 4   year          346440 non-null  int64  \n",
            " 5   month         346440 non-null  int64  \n",
            " 6   day           346440 non-null  int64  \n",
            " 7   power         329304 non-null  float64\n",
            "dtypes: float64(1), int64(7)\n",
            "memory usage: 23.8 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#forward_fill: Missing values ​​are filled according to the previous value\n",
        "total_df.fillna(method='ffill', inplace=True)\n",
        "total_df[\"day_sin\"] = np.sin(total_df[\"day\"]*(2*np.pi / 30))\n",
        "total_df[\"day_cos\"] = np.cos(total_df[\"day\"]*(2*np.pi / 30))\n",
        "total_df['sin_month']=np.sin(2*np.pi*total_df['month']/12)\n",
        "total_df['cos_month']=np.cos(2*np.pi*total_df['month']/12)\n",
        "total_df['sin_hour']=np.sin(2*np.pi*total_df['hour']/24)\n",
        "total_df['cos_hour']=np.cos(2*np.pi*total_df['hour']/24)\n",
        "total_df.drop(columns= [\"day\", \"month\",\"hour\"], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "mkL5NuUDVH_q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_df.drop(columns=[\"id\", \"year\", \"parking_free\"], inplace= True)\n",
        "print(total_df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QyLnGHefIDZ",
        "outputId": "e3efe38f-c7c0-41ad-dae6-8725e4c0e06b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 346440 entries, 0 to 17135\n",
            "Data columns (total 8 columns):\n",
            " #   Column     Non-Null Count   Dtype  \n",
            "---  ------     --------------   -----  \n",
            " 0   id_encode  346440 non-null  int64  \n",
            " 1   power      346440 non-null  float64\n",
            " 2   day_sin    346440 non-null  float64\n",
            " 3   day_cos    346440 non-null  float64\n",
            " 4   sin_month  346440 non-null  float64\n",
            " 5   cos_month  346440 non-null  float64\n",
            " 6   sin_hour   346440 non-null  float64\n",
            " 7   cos_hour   346440 non-null  float64\n",
            "dtypes: float64(7), int64(1)\n",
            "memory usage: 23.8 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and test Data Frames\n",
        "extracing test_df and train_df from our total_df with new features"
      ],
      "metadata": {
        "id": "d3_cIGcjiRMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_df=total_df[:len(train_df)]\n",
        "prediction_df=total_df[len(train_df):]\n",
        "\n",
        "print(f\"train set length : {len(main_df)}\\nprediction set length: {len(prediction_df)}\")"
      ],
      "metadata": {
        "id": "ln1389R_iuuC",
        "outputId": "1d569b96-e583-4d18-954e-930c2b5a296f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set length : 329304\n",
            "prediction set length: 17136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## normalizing the power column\n",
        "Normalizing using the Standard Deviation Normalization method"
      ],
      "metadata": {
        "id": "7QU807vphf2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_df_power_mean = main_df[\"power\"].mean()\n",
        "main_df_power_std = main_df[\"power\"].std()"
      ],
      "metadata": {
        "id": "go0ci8rkE06d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_df[\"power\"] = (main_df[\"power\"] - main_df_power_mean) / main_df_power_std"
      ],
      "metadata": {
        "id": "zVGRekT8fuqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38f3f96e-c012-46ee-f947-55b431a16fdd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-8639e7bc6d6d>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  main_df[\"power\"] = (main_df[\"power\"] - main_df_power_mean) / main_df_power_std\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting Ensemble Method\n",
        "using LGBMRegressor for predictions"
      ],
      "metadata": {
        "id": "XQFzfMIXkqfN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-03T14:30:28.711927Z",
          "iopub.status.busy": "2023-10-03T14:30:28.711493Z",
          "iopub.status.idle": "2023-10-03T14:30:30.529421Z",
          "shell.execute_reply": "2023-10-03T14:30:30.528233Z",
          "shell.execute_reply.started": "2023-10-03T14:30:28.711895Z"
        },
        "id": "yXd_B6oW1MZ0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold #Import the k-fold cross validation function in the machine learning library\n",
        "from lightgbm import LGBMRegressor #Import the integrated learning algorithm lightgbm\n",
        "import lightgbm as lgb\n",
        "def RMSE(y_true,y_pred):\n",
        "    return np.sqrt(np.mean((y_true-y_pred)**2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tunning Hyperparameters\n",
        "Tuning hypereparameters of the lgm model using bayesian optimization"
      ],
      "metadata": {
        "id": "hu6czTdhza9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install scikit-optimize\n",
        "!pip install matplotlib\n",
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "id": "-dAcD2wL2Hia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## importing libraries\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# laoding dataset\n",
        "y = main_df['power']\n",
        "X = main_df.drop(['power'],axis=1)\n",
        "\n",
        "# defining the optimization function for the bayesian Optimization\n",
        "def lgbm_cv(num_leaves, learning_rate, n_estimators, subsample, colsample_bytree, reg_alpha, reg_lambda):\n",
        "    params = {\n",
        "        'num_leaves': int(num_leaves),\n",
        "        'learning_rate': learning_rate,\n",
        "        'n_estimators': int(n_estimators),\n",
        "        'subsample': subsample,\n",
        "        'colsample_bytree': colsample_bytree,\n",
        "        'reg_alpha': reg_alpha,\n",
        "        'reg_lambda': reg_lambda,\n",
        "        'objective': 'regression',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    # Set up k-fold cross-validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # Create and train the LGBMRegressor model\n",
        "        model = lgb.LGBMRegressor(**params)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        score = model.score(X_val, y_val)\n",
        "        scores.append(score)\n",
        "\n",
        "    # Calculate the mean score across folds\n",
        "    mean_score = np.mean(scores)\n",
        "\n",
        "    return mean_score\n"
      ],
      "metadata": {
        "id": "n6ujFGx-zas0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the search space for hyperparameters\n",
        "pbounds = {\n",
        "    'num_leaves': (50, 400),\n",
        "    'learning_rate': (0.01, 0.3),\n",
        "    'n_estimators': (50, 200),\n",
        "    'subsample': (0.8, 1.0),\n",
        "    'colsample_bytree': (0.8, 1.0),\n",
        "    'reg_alpha': (0.0, 1.0),\n",
        "    'reg_lambda': (0.0, 1.0),\n",
        "}\n",
        "\n",
        "# Initialize BayesianOptimization\n",
        "lgbm_bo = BayesianOptimization(f=lgbm_cv, pbounds=pbounds, random_state=42)\n",
        "\n",
        "# Perform Bayesian Optimization\n",
        "init_points = 10\n",
        "n_iter = 100\n",
        "lgbm_bo.maximize(init_points=init_points, n_iter=n_iter)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = lgbm_bo.max['params']\n",
        "best_params['num_leaves'] = int(best_params['num_leaves'])\n",
        "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "06KLy9--_SFX",
        "outputId": "c8b5f3d3-95aa-4231-c18b-4b040fca4447"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | colsam... | learni... | n_esti... | num_le... | reg_alpha | reg_la... | subsample |\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020099 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017417 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068986 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017728 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017864 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.8603   \u001b[0m | \u001b[0m0.8749   \u001b[0m | \u001b[0m0.2857   \u001b[0m | \u001b[0m159.8    \u001b[0m | \u001b[0m259.5    \u001b[0m | \u001b[0m0.156    \u001b[0m | \u001b[0m0.156    \u001b[0m | \u001b[0m0.8116   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024418 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005848 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005725 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005992 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m0.8501   \u001b[0m | \u001b[0m0.9732   \u001b[0m | \u001b[0m0.1843   \u001b[0m | \u001b[0m156.2    \u001b[0m | \u001b[0m57.2     \u001b[0m | \u001b[0m0.9699   \u001b[0m | \u001b[0m0.8324   \u001b[0m | \u001b[0m0.8425   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021330 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123994 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027649 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020057 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022548 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m0.8516   \u001b[0m | \u001b[0m0.8364   \u001b[0m | \u001b[0m0.06319  \u001b[0m | \u001b[0m95.64    \u001b[0m | \u001b[0m233.7    \u001b[0m | \u001b[0m0.4319   \u001b[0m | \u001b[0m0.2912   \u001b[0m | \u001b[0m0.9224   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024237 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006443 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019850 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019532 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017057 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.8547   \u001b[0m | \u001b[0m0.8279   \u001b[0m | \u001b[0m0.09472  \u001b[0m | \u001b[0m105.0    \u001b[0m | \u001b[0m209.6    \u001b[0m | \u001b[0m0.7852   \u001b[0m | \u001b[0m0.1997   \u001b[0m | \u001b[0m0.9028   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022536 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019599 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017563 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037534 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017044 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m0.8102   \u001b[0m | \u001b[0m0.9185   \u001b[0m | \u001b[0m0.02347  \u001b[0m | \u001b[0m141.1    \u001b[0m | \u001b[0m109.7    \u001b[0m | \u001b[0m0.06505  \u001b[0m | \u001b[0m0.9489   \u001b[0m | \u001b[0m0.9931   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010227 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005743 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005730 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005775 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006015 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m0.8572   \u001b[0m | \u001b[0m0.9617   \u001b[0m | \u001b[0m0.09834  \u001b[0m | \u001b[0m64.65    \u001b[0m | \u001b[0m289.5    \u001b[0m | \u001b[0m0.4402   \u001b[0m | \u001b[0m0.122    \u001b[0m | \u001b[0m0.899    \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029136 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017662 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019836 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018124 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017199 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m0.8597   \u001b[0m | \u001b[0m0.8069   \u001b[0m | \u001b[0m0.2737   \u001b[0m | \u001b[0m88.82    \u001b[0m | \u001b[0m281.9    \u001b[0m | \u001b[0m0.3117   \u001b[0m | \u001b[0m0.5201   \u001b[0m | \u001b[0m0.9093   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005968 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018086 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018793 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023371 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018326 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[95m8        \u001b[0m | \u001b[95m0.8609   \u001b[0m | \u001b[95m0.837    \u001b[0m | \u001b[95m0.2912   \u001b[0m | \u001b[95m166.3    \u001b[0m | \u001b[95m378.8    \u001b[0m | \u001b[95m0.8948   \u001b[0m | \u001b[95m0.5979   \u001b[0m | \u001b[95m0.9844   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021655 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018080 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019828 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017667 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018007 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m0.8311   \u001b[0m | \u001b[0m0.8177   \u001b[0m | \u001b[0m0.06684  \u001b[0m | \u001b[0m56.78    \u001b[0m | \u001b[0m163.9    \u001b[0m | \u001b[0m0.3887   \u001b[0m | \u001b[0m0.2713   \u001b[0m | \u001b[0m0.9657   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022300 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017122 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017099 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017159 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020216 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m0.8451   \u001b[0m | \u001b[0m0.8714   \u001b[0m | \u001b[0m0.09147  \u001b[0m | \u001b[0m131.4    \u001b[0m | \u001b[0m99.32    \u001b[0m | \u001b[0m0.8022   \u001b[0m | \u001b[0m0.07455  \u001b[0m | \u001b[0m0.9974   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020718 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017509 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018790 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020013 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017305 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m0.8599   \u001b[0m | \u001b[0m0.8798   \u001b[0m | \u001b[0m0.2535   \u001b[0m | \u001b[0m78.28    \u001b[0m | \u001b[0m285.4    \u001b[0m | \u001b[0m0.977    \u001b[0m | \u001b[0m0.1019   \u001b[0m | \u001b[0m0.8017   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006977 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126987 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020274 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017645 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017615 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m0.7862   \u001b[0m | \u001b[0m0.832    \u001b[0m | \u001b[0m0.01179  \u001b[0m | \u001b[0m161.0    \u001b[0m | \u001b[0m260.2    \u001b[0m | \u001b[0m0.5031   \u001b[0m | \u001b[0m0.456    \u001b[0m | \u001b[0m0.8995   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017658 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018438 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026572 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017436 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018098 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[95m13       \u001b[0m | \u001b[95m0.8619   \u001b[0m | \u001b[95m0.9131   \u001b[0m | \u001b[95m0.2098   \u001b[0m | \u001b[95m159.3    \u001b[0m | \u001b[95m259.1    \u001b[0m | \u001b[95m0.7697   \u001b[0m | \u001b[95m0.2293   \u001b[0m | \u001b[95m0.9747   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112103 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022650 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020829 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020078 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144993 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m0.8614   \u001b[0m | \u001b[0m0.8556   \u001b[0m | \u001b[0m0.232    \u001b[0m | \u001b[0m159.3    \u001b[0m | \u001b[0m258.4    \u001b[0m | \u001b[0m0.4842   \u001b[0m | \u001b[0m0.7913   \u001b[0m | \u001b[0m0.8076   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017987 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017886 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130111 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017173 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020207 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m0.828    \u001b[0m | \u001b[0m0.927    \u001b[0m | \u001b[0m0.01808  \u001b[0m | \u001b[0m158.3    \u001b[0m | \u001b[0m258.3    \u001b[0m | \u001b[0m0.6746   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m0.9005   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017519 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019448 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023518 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020157 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020442 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m16       \u001b[0m | \u001b[0m0.8584   \u001b[0m | \u001b[0m0.9233   \u001b[0m | \u001b[0m0.06819  \u001b[0m | \u001b[0m159.6    \u001b[0m | \u001b[0m259.1    \u001b[0m | \u001b[0m0.1399   \u001b[0m | \u001b[0m0.3711   \u001b[0m | \u001b[0m0.9277   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019654 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017788 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018799 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017969 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017570 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[95m17       \u001b[0m | \u001b[95m0.8623   \u001b[0m | \u001b[95m0.8147   \u001b[0m | \u001b[95m0.192    \u001b[0m | \u001b[95m165.3    \u001b[0m | \u001b[95m379.4    \u001b[0m | \u001b[95m0.3937   \u001b[0m | \u001b[95m0.9117   \u001b[0m | \u001b[95m0.8967   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019138 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017633 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018115 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018228 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000112\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018408 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263444, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.000397\n",
            "| \u001b[0m18       \u001b[0m | \u001b[0m0.8609   \u001b[0m | \u001b[0m0.8317   \u001b[0m | \u001b[0m0.2878   \u001b[0m | \u001b[0m166.0    \u001b[0m | \u001b[0m379.7    \u001b[0m | \u001b[0m0.7986   \u001b[0m | \u001b[0m0.7521   \u001b[0m | \u001b[0m0.8294   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005730 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000563\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005836 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001119\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005806 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 263443, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.000842\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1ea5225601b8>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0minit_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mlgbm_bo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Get the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-88f4fb5f3305>\u001b[0m in \u001b[0;36mlgbm_cv\u001b[0;34m(num_leaves, learning_rate, n_estimators, subsample, colsample_bytree, reg_alpha, reg_lambda)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Create and train the LGBMRegressor model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Evaluate the model on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     ) -> \"LGBMRegressor\":\n\u001b[1;32m   1048\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1050\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    843\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meval_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_training_booster\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbooster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mmodel_to_string\u001b[0;34m(self, num_iteration, start_iteration, importance_type)\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0mstring_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_string_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3970\u001b[0m             \u001b[0mptr_string_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddressof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3971\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterSaveModelToString(\n\u001b[0m\u001b[1;32m   3972\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3973\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-10-03T14:30:30.531562Z",
          "iopub.status.busy": "2023-10-03T14:30:30.531196Z",
          "iopub.status.idle": "2023-10-03T14:53:34.219508Z",
          "shell.execute_reply": "2023-10-03T14:53:34.218379Z",
          "shell.execute_reply.started": "2023-10-03T14:30:30.531533Z"
        },
        "id": "z3f7ONYQ1MZ0",
        "outputId": "5760823c-1127-4f38-b06f-5bf9d3307489",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start fit....\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011499 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296373, number of used features: 7\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score -0.000389\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012964 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296373, number of used features: 7\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 0.000891\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008273 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296373, number of used features: 7\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score -0.000059\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008330 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296373, number of used features: 7\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score -0.000944\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008319 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296374, number of used features: 7\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score -0.000363\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012265 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296374, number of used features: 7\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 0.001121\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008572 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296374, number of used features: 7\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score -0.000311\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008427 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296374, number of used features: 7\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 0.000408\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038003 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296374, number of used features: 7\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 0.000071\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008269 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296374, number of used features: 7\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score -0.000424\n",
            "mean_train_RMSE:0.3314637247225325\n",
            "mean_valid_RMSE:0.370541675566491\n"
          ]
        }
      ],
      "source": [
        "print(\"start fit....\")\n",
        "folds = 10 #Divide the data into 10 parts\n",
        "y = main_df['power']\n",
        "X = main_df.drop(['power'],axis=1)\n",
        "\n",
        "train_RMSE=[]\n",
        "valid_RMSE=[]\n",
        "\n",
        "# Store the list of learned models\n",
        "models = []\n",
        "\n",
        "#Shuffle the data set randomly and divide it into folds\n",
        "kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
        "\n",
        "#Divide x_train into a training set and a verification set in a ratio of 9:1, and take out the subscripts\n",
        "for train_index, valid_index in kf.split(X):\n",
        "\n",
        "    #Get the data of the training set and validation set based on the subscripts\n",
        "    x_train_cv = X.iloc[train_index]\n",
        "    y_train_cv = y.iloc[train_index]\n",
        "    x_valid_cv =X.iloc[valid_index]\n",
        "    y_valid_cv = y.iloc[valid_index]\n",
        "\n",
        "    #Call the LightGBM regression model and add parameters\n",
        "    model = LGBMRegressor(**best_params)\n",
        "\n",
        "    #Use x_train_cv to train the model, and use x_train_cv and x_valid_cv to evaluate together\n",
        "    model.fit(\n",
        "        x_train_cv,\n",
        "        y_train_cv,\n",
        "        eval_set = [(x_train_cv, y_train_cv), (x_valid_cv, y_valid_cv)],\n",
        "        #verbose = 100, #Iterate 100 times and output a result\n",
        "    )\n",
        "\n",
        "    #Predict the training set\n",
        "    y_pred_train = model.predict(x_train_cv, num_iteration=model.best_iteration_)\n",
        "    #Predict on the validation set\n",
        "    y_pred_valid = model.predict(x_valid_cv, num_iteration=model.best_iteration_)\n",
        "\n",
        "    train_rmse=RMSE(y_pred_train,y_train_cv)\n",
        "    valid_rmse=RMSE(y_pred_valid,y_valid_cv)\n",
        "\n",
        "    train_RMSE.append(train_rmse)\n",
        "    valid_RMSE.append(valid_rmse)\n",
        "    #Save model into list\n",
        "    models.append(model)\n",
        "    #print(f\"train_RMSE:{train_RMSE},valid_RMSE:{valid_RMSE}\")\n",
        "\n",
        "train_RMSE=np.array(train_RMSE)\n",
        "valid_RMSE=np.array(valid_RMSE)\n",
        "\n",
        "print(f\"mean_train_RMSE:{np.mean(train_RMSE)}\")\n",
        "print(f\"mean_valid_RMSE:{np.mean(valid_RMSE)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-03T14:53:34.223277Z",
          "iopub.status.busy": "2023-10-03T14:53:34.220831Z",
          "iopub.status.idle": "2023-10-03T14:53:42.156365Z",
          "shell.execute_reply": "2023-10-03T14:53:42.155235Z",
          "shell.execute_reply.started": "2023-10-03T14:53:34.223230Z"
        },
        "id": "BgFmECi01MZ1",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "262c5a74-ab98-4a37-fb2b-a042ffd80d6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 1.17034853, 2.32652798, 5.178213  ,\n",
              "       3.99238684, 1.38790999, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 1.18549959, 2.44841415, 5.42407367, 4.24353637,\n",
              "       1.5783384 , 0.        , 0.        , 0.        , 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "test_X = prediction_df.drop(['power'],axis=1).values\n",
        "#Use each saved model to predict x_test once, and then take the average\n",
        "preds_test = []\n",
        "\n",
        "for model in models:\n",
        "\n",
        "    pred = model.predict(test_X, num_iteration=model.best_iteration_)\n",
        "\n",
        "    preds_test.append(pred)\n",
        "\n",
        "# Reverse the normalization\n",
        "original_predictions = preds_test * total_df[\"power\"].std() + total_df[\"power\"].mean()\n",
        "#Convert the prediction results into np.array\n",
        "preds_test_np = np.array(original_predictions)\n",
        "#Average the prediction results of each model by column\n",
        "test_pred= preds_test_np.mean(axis=0)\n",
        "test_pred=np.where(test_pred<=0,0,test_pred)\n",
        "test_pred[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-03T14:53:42.158252Z",
          "iopub.status.busy": "2023-10-03T14:53:42.157836Z",
          "iopub.status.idle": "2023-10-03T14:53:42.230039Z",
          "shell.execute_reply": "2023-10-03T14:53:42.228945Z",
          "shell.execute_reply.started": "2023-10-03T14:53:42.158214Z"
        },
        "id": "1-Art-s01MZ1",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "2f536344-6a31-4db1-dff0-42b5e4f859c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  power\n",
              "0  8401    0.0\n",
              "1  8402    0.0\n",
              "2  8403    0.0\n",
              "3  8404    0.0\n",
              "4  8405    0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0be7f542-606c-47b9-882c-2c43c2fbe930\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>power</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8401</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8402</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8403</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8404</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8405</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0be7f542-606c-47b9-882c-2c43c2fbe930')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0be7f542-606c-47b9-882c-2c43c2fbe930 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0be7f542-606c-47b9-882c-2c43c2fbe930');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-476f8c94-199f-4c86-889a-c83cf5eab773\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-476f8c94-199f-4c86-889a-c83cf5eab773')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-476f8c94-199f-4c86-889a-c83cf5eab773 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "submission=pd.read_csv(\"sample_submission.csv\")\n",
        "submission['power']=test_pred\n",
        "submission.to_csv(\"baseline.csv\",index=None)\n",
        "submission.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network model\n",
        "Using a Multi Layer model for predection"
      ],
      "metadata": {
        "id": "J_KLcDGBqXG1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66Ot1pELDl8v",
        "outputId": "8105adcc-1de2-4a6e-c3bb-b66fa635b422",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0dd832cbb0>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Test split"
      ],
      "metadata": {
        "id": "GUK4_QKNQsgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_test_ratio = 0.8\n",
        "train_dataset = main_df[ : int(len(train_df)*train_test_ratio)]\n",
        "test_dataset = main_df[int(len(train_df)*train_test_ratio) : ]"
      ],
      "metadata": {
        "id": "5Xn7NQgrQxS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wEwxZLgHwxR"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.Tensor(train_dataset.drop(['power'],axis=1).values.astype(np.float32))\n",
        "y_train_tensor = torch.Tensor(train_dataset['power'].values.astype(np.float32))\n",
        "\n",
        "X_test_tensor = torch.Tensor(test_dataset.drop(['power'],axis=1).values.astype(np.float32))\n",
        "y_test_tensor = torch.Tensor(test_dataset['power'].values.astype(np.float32))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor.shape , y_train_tensor.shape"
      ],
      "metadata": {
        "id": "vYSBua84R2De",
        "outputId": "ef531dd2-06d9-44e1-d3da-d0e4f8cd0372",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([210528, 7]), torch.Size([210528]))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbDd2kcyJTGU"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgKSPZGYJck3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiJHj_nZJhrs"
      },
      "outputs": [],
      "source": [
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, input_features, hidden_units, output_size):\n",
        "        super().__init__()\n",
        "        self.linear_layer_stack = nn.Sequential(\n",
        "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units, out_features=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=4, out_features=output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layer_stack(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWtI_sbiKb-o"
      },
      "outputs": [],
      "source": [
        "nn_model = RegressionModel(input_features= X_train_tensor.shape[1],\n",
        "                    output_size=1,\n",
        "                    hidden_units= 8).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AQS2hVqLdTo"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
        "optimizer = optim.AdamW(nn_model.parameters(), lr=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jR8o-42S5cI",
        "outputId": "04eed6f2-5f90-4ee5-9bc2-ce9e50d3e0ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output shape =  torch.Size([128, 1])\n",
            "targets shape =  torch.Size([128, 1])\n",
            "loss value =  1.1553386449813843\n"
          ]
        }
      ],
      "source": [
        "## try a single run on the model to check the output shapes\n",
        "nn_model.train()\n",
        "for inputs, targets in train_loader:\n",
        "    output = nn_model(inputs)\n",
        "    print(\"output shape = \", output.shape)\n",
        "    targets = targets.unsqueeze(dim=1)\n",
        "    print(\"targets shape = \", targets.shape)\n",
        "    loss = criterion(output, targets)\n",
        "    print(\"loss value = \", loss.item())\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xkkD3aYMCFC",
        "outputId": "22de5250-f9f0-4755-b0aa-9cab699d4d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "-------\n",
            "\n",
            "Train loss: 0.79628 | Test loss: 0.98649\n",
            "Epoch: 1\n",
            "-------\n",
            "\n",
            "Train loss: 0.75656 | Test loss: 1.03925\n",
            "Epoch: 2\n",
            "-------\n",
            "\n",
            "Train loss: 0.74396 | Test loss: 1.05232\n",
            "Epoch: 3\n",
            "-------\n",
            "\n",
            "Train loss: 0.73867 | Test loss: 1.13611\n",
            "Epoch: 4\n",
            "-------\n",
            "\n",
            "Train loss: 0.73675 | Test loss: 1.07861\n",
            "Epoch: 5\n",
            "-------\n",
            "\n",
            "Train loss: 0.73654 | Test loss: 1.07169\n",
            "Epoch: 6\n",
            "-------\n",
            "\n",
            "Train loss: 0.73681 | Test loss: 1.16946\n",
            "Epoch: 7\n",
            "-------\n",
            "\n",
            "Train loss: 0.73753 | Test loss: 1.05498\n",
            "Epoch: 8\n",
            "-------\n",
            "\n",
            "Train loss: 0.73740 | Test loss: 1.14926\n",
            "Epoch: 9\n",
            "-------\n",
            "\n",
            "Train loss: 0.73784 | Test loss: 1.04795\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Epoch: {epoch}\\n-------\")\n",
        "\n",
        "  nn_model.train()\n",
        "  ### Training\n",
        "  train_loss = 0\n",
        "  for batch, (inputs, targets) in enumerate(train_loader):\n",
        "    targets = targets.unsqueeze(dim=1)\n",
        "    # 1. Forward pass\n",
        "    outputs = nn_model(inputs)\n",
        "    # 2. Calculate loss (per batch)\n",
        "    loss = torch.sqrt(criterion(outputs, targets))\n",
        "    train_loss += loss # accumulatively add up the loss per epoch\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "    #if batch % 400 == 0:\n",
        "      #print(f\"Looked at {batch * len(inputs)}/{len(train_loader.dataset)} samples\")\n",
        "\n",
        "  # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
        "  train_loss /= len(train_loader)\n",
        "  ### Testing\n",
        "  # Setup variables for accumulatively adding up loss and accuracy\n",
        "  test_loss = 0\n",
        "  nn_model.eval()\n",
        "  with torch.inference_mode():\n",
        "    rmse_sum = 0.0\n",
        "    num_samples = 0\n",
        "    for inputs, targets in test_loader:\n",
        "      targets = targets.unsqueeze(dim=1)\n",
        "      # 1. Forward pass\n",
        "      test_outputs = nn_model(inputs)\n",
        "      # 2. Calculate loss (accumatively)\n",
        "      test_loss += torch.sqrt(criterion(test_outputs, targets))\n",
        "\n",
        "    # Divide total test loss by length of test dataloader (per epoch)\n",
        "    test_loss /= len(test_loader)\n",
        "  ## Print out what's happening\n",
        "  print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}