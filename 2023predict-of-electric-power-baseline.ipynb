{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data manipulation\n",
        "includes importing the data and feature engineering"
      ],
      "metadata": {
        "id": "82Ta13bpPaWe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-10-03T14:30:04.545342Z",
          "iopub.status.busy": "2023-10-03T14:30:04.544551Z",
          "iopub.status.idle": "2023-10-03T14:30:04.550390Z",
          "shell.execute_reply": "2023-10-03T14:30:04.549132Z",
          "shell.execute_reply.started": "2023-10-03T14:30:04.545302Z"
        },
        "id": "us9UYG1E1MZv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Library for importing csv files\n",
        "import pandas as pd\n",
        "#Library for matrix operations\n",
        "import numpy as np\n",
        "#Set random seeds to ensure that the model can be reproduced\n",
        "import random\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## importing training data and test data\n",
        "\n",
        "- important note: the the test dataframe refers to the dataset which we want to predict the values and measure the RMSE, its not involved in the training and test phase. It is used after the traing is done."
      ],
      "metadata": {
        "id": "q1NofqdGQGC5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-10-03T14:30:04.556580Z",
          "iopub.status.busy": "2023-10-03T14:30:04.555843Z",
          "iopub.status.idle": "2023-10-03T14:30:05.017935Z",
          "shell.execute_reply": "2023-10-03T14:30:05.016306Z",
          "shell.execute_reply.started": "2023-10-03T14:30:04.556545Z"
        },
        "id": "olNTK4Ep1MZx",
        "outputId": "396453f2-b75b-4385-bbe7-48cf3ca381f0",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_df):329304\n",
            "len(test_df):17136\n"
          ]
        }
      ],
      "source": [
        "# importing data which is in .csv format\n",
        "train_df=pd.read_csv(\"train_df.csv\")\n",
        "print(f\"len(train_df):{len(train_df)}\")\n",
        "test_df=pd.read_csv(\"test_df.csv\")\n",
        "print(f\"len(test_df):{len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concatinating the training and test data frames for futher data manipulation\n",
        "total_df=pd.concat((train_df,test_df),axis=0)\n",
        "# dropping f1 and f2 columns in the dataset since it is not informative\n",
        "total_df.drop([\"f1\",\"f2\"], axis=1,inplace= True)\n",
        "print(total_df.info())"
      ],
      "metadata": {
        "id": "S4VAcBpKQrie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ae598fa-f794-4ec6-c24b-c55fa03834d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 346440 entries, 0 to 17135\n",
            "Data columns (total 8 columns):\n",
            " #   Column        Non-Null Count   Dtype  \n",
            "---  ------        --------------   -----  \n",
            " 0   id            346440 non-null  int64  \n",
            " 1   id_encode     346440 non-null  int64  \n",
            " 2   hour          346440 non-null  int64  \n",
            " 3   parking_free  346440 non-null  int64  \n",
            " 4   year          346440 non-null  int64  \n",
            " 5   month         346440 non-null  int64  \n",
            " 6   day           346440 non-null  int64  \n",
            " 7   power         329304 non-null  float64\n",
            "dtypes: float64(1), int64(7)\n",
            "memory usage: 23.8 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#forward_fill: Missing values ​​are filled according to the previous value\n",
        "total_df.fillna(method='ffill', inplace=True)\n",
        "total_df[\"day_sin\"] = np.sin(total_df[\"day\"]*(2*np.pi / 30))\n",
        "total_df[\"day_cos\"] = np.cos(total_df[\"day\"]*(2*np.pi / 30))\n",
        "total_df['sin_month']=np.sin(2*np.pi*total_df['month']/12)\n",
        "total_df['cos_month']=np.cos(2*np.pi*total_df['month']/12)\n",
        "total_df['sin_hour']=np.sin(2*np.pi*total_df['hour']/24)\n",
        "total_df['cos_hour']=np.cos(2*np.pi*total_df['hour']/24)\n",
        "total_df.drop(columns= [\"day\", \"month\",\"hour\"], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "mkL5NuUDVH_q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_df.drop(columns=[\"id\", \"year\", \"parking_free\"], inplace= True)\n",
        "print(total_df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QyLnGHefIDZ",
        "outputId": "e7833688-445c-4e21-9a26-0509c5e00f5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 346440 entries, 0 to 17135\n",
            "Data columns (total 8 columns):\n",
            " #   Column     Non-Null Count   Dtype  \n",
            "---  ------     --------------   -----  \n",
            " 0   id_encode  346440 non-null  int64  \n",
            " 1   power      346440 non-null  float64\n",
            " 2   day_sin    346440 non-null  float64\n",
            " 3   day_cos    346440 non-null  float64\n",
            " 4   sin_month  346440 non-null  float64\n",
            " 5   cos_month  346440 non-null  float64\n",
            " 6   sin_hour   346440 non-null  float64\n",
            " 7   cos_hour   346440 non-null  float64\n",
            "dtypes: float64(7), int64(1)\n",
            "memory usage: 23.8 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and test Data Frames\n",
        "extracing test_df and train_df from our total_df with new features"
      ],
      "metadata": {
        "id": "d3_cIGcjiRMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_df=total_df[:len(train_df)]\n",
        "prediction_df=total_df[len(train_df):]\n",
        "\n",
        "print(f\"train set length : {len(main_df)}\\nprediction set length: {len(prediction_df)}\")"
      ],
      "metadata": {
        "id": "ln1389R_iuuC",
        "outputId": "bf057dbc-bbfc-463e-e098-6018bca894f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set length : 329304\n",
            "prediction set length: 17136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## normalizing the power column\n",
        "Normalizing using the Standard Deviation Normalization method"
      ],
      "metadata": {
        "id": "7QU807vphf2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_df_power_mean = main_df[\"power\"].mean()\n",
        "main_df_power_std = main_df[\"power\"].std()"
      ],
      "metadata": {
        "id": "go0ci8rkE06d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_df.loc[:, \"power\"] = (main_df[\"power\"] - main_df_power_mean) / main_df_power_std"
      ],
      "metadata": {
        "id": "zVGRekT8fuqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c380ff8-4ea5-45d1-f1b2-67afa50d93e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-e89033bf959e>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  main_df.loc[:, \"power\"] = (main_df[\"power\"] - main_df_power_mean) / main_df_power_std\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting Ensemble Method\n",
        "using LGBMRegressor for predictions"
      ],
      "metadata": {
        "id": "XQFzfMIXkqfN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-03T14:30:28.711927Z",
          "iopub.status.busy": "2023-10-03T14:30:28.711493Z",
          "iopub.status.idle": "2023-10-03T14:30:30.529421Z",
          "shell.execute_reply": "2023-10-03T14:30:30.528233Z",
          "shell.execute_reply.started": "2023-10-03T14:30:28.711895Z"
        },
        "id": "yXd_B6oW1MZ0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold #Import the k-fold cross validation function in the machine learning library\n",
        "from lightgbm import LGBMRegressor #Import the integrated learning algorithm lightgbm\n",
        "import lightgbm as lgb\n",
        "def RMSE(y_true,y_pred):\n",
        "    return np.sqrt(np.mean((y_true-y_pred)**2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tunning Hyperparameters\n",
        "Tuning hypereparameters of the lgm model using bayesian optimization"
      ],
      "metadata": {
        "id": "hu6czTdhza9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install scikit-learn\n",
        "#!pip install scikit-optimize\n",
        "#!pip install matplotlib\n",
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "id": "-dAcD2wL2Hia",
        "outputId": "c75f31a6-9f32-4e6e-f4eb-f9d599069199",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.2.0)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## importing libraries\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# laoding dataset\n",
        "y = main_df['power']\n",
        "X = main_df.drop(['power'],axis=1)\n",
        "\n",
        "# defining the optimization function for the bayesian Optimization\n",
        "def lgbm_cv(num_leaves, learning_rate, n_estimators, subsample, colsample_bytree, reg_alpha, reg_lambda):\n",
        "    params = {\n",
        "        'num_leaves': int(num_leaves),\n",
        "        'learning_rate': learning_rate,\n",
        "        'n_estimators': int(n_estimators),\n",
        "        'subsample': subsample,\n",
        "        'colsample_bytree': colsample_bytree,\n",
        "        'reg_alpha': reg_alpha,\n",
        "        'reg_lambda': reg_lambda,\n",
        "        'objective': 'regression',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'random_state': 42,\n",
        "        'metric': 'RMSE',\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    # Set up k-fold cross-validation\n",
        "    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # Create and train the LGBMRegressor model\n",
        "        model = lgb.LGBMRegressor(**params)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        score = model.score(X_val, y_val)\n",
        "        scores.append(score)\n",
        "\n",
        "    # Calculate the mean score across folds\n",
        "    mean_score = np.mean(scores)\n",
        "\n",
        "    return mean_score\n"
      ],
      "metadata": {
        "id": "n6ujFGx-zas0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the search space for hyperparameters\n",
        "parameter_bounds = {\n",
        "    'num_leaves': (50, 300),\n",
        "    'learning_rate': (0.001, 0.1),\n",
        "    'n_estimators': (50, 500),\n",
        "    'subsample': (0.8, 1.0),\n",
        "    'colsample_bytree': (0.8, 1.0),\n",
        "    'reg_alpha': (0.0, 1.0),\n",
        "    'reg_lambda': (0.0, 1.0),\n",
        "}\n",
        "\n",
        "# Initialize BayesianOptimization\n",
        "lgbm_bo = BayesianOptimization(f=lgbm_cv, pbounds=parameter_bounds, random_state=42)\n",
        "\n",
        "# Perform Bayesian Optimization\n",
        "init_points = 10\n",
        "n_iter = 50\n",
        "lgbm_bo.maximize(init_points=init_points, n_iter=n_iter)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = lgbm_bo.max['params']\n",
        "best_params['num_leaves'] = int(best_params['num_leaves'])\n",
        "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06KLy9--_SFX",
        "outputId": "173115cd-a6e5-42f6-d023-3b8c37213e5c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | colsam... | learni... | n_esti... | num_le... | reg_alpha | reg_la... | subsample |\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003627 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109203 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.8569   \u001b[0m | \u001b[0m0.8749   \u001b[0m | \u001b[0m0.09512  \u001b[0m | \u001b[0m379.4    \u001b[0m | \u001b[0m199.7    \u001b[0m | \u001b[0m0.156    \u001b[0m | \u001b[0m0.156    \u001b[0m | \u001b[0m0.8116   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003705 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003725 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m0.8428   \u001b[0m | \u001b[0m0.9732   \u001b[0m | \u001b[0m0.06051  \u001b[0m | \u001b[0m368.6    \u001b[0m | \u001b[0m55.15    \u001b[0m | \u001b[0m0.9699   \u001b[0m | \u001b[0m0.8324   \u001b[0m | \u001b[0m0.8425   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011323 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003531 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m0.8306   \u001b[0m | \u001b[0m0.8364   \u001b[0m | \u001b[0m0.01916  \u001b[0m | \u001b[0m186.9    \u001b[0m | \u001b[0m181.2    \u001b[0m | \u001b[0m0.4319   \u001b[0m | \u001b[0m0.2912   \u001b[0m | \u001b[0m0.9224   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003536 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090778 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.8451   \u001b[0m | \u001b[0m0.8279   \u001b[0m | \u001b[0m0.02992  \u001b[0m | \u001b[0m214.9    \u001b[0m | \u001b[0m164.0    \u001b[0m | \u001b[0m0.7852   \u001b[0m | \u001b[0m0.1997   \u001b[0m | \u001b[0m0.9028   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003559 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101207 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m0.739    \u001b[0m | \u001b[0m0.9185   \u001b[0m | \u001b[0m0.005599 \u001b[0m | \u001b[0m323.4    \u001b[0m | \u001b[0m92.63    \u001b[0m | \u001b[0m0.06505  \u001b[0m | \u001b[0m0.9489   \u001b[0m | \u001b[0m0.9931   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003655 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003682 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m0.8319   \u001b[0m | \u001b[0m0.9617   \u001b[0m | \u001b[0m0.03116  \u001b[0m | \u001b[0m93.95    \u001b[0m | \u001b[0m221.1    \u001b[0m | \u001b[0m0.4402   \u001b[0m | \u001b[0m0.122    \u001b[0m | \u001b[0m0.899    \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105094 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003535 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m0.8542   \u001b[0m | \u001b[0m0.8069   \u001b[0m | \u001b[0m0.09102  \u001b[0m | \u001b[0m166.5    \u001b[0m | \u001b[0m215.6    \u001b[0m | \u001b[0m0.3117   \u001b[0m | \u001b[0m0.5201   \u001b[0m | \u001b[0m0.9093   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012308 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003599 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[95m8        \u001b[0m | \u001b[95m0.8573   \u001b[0m | \u001b[95m0.837    \u001b[0m | \u001b[95m0.09699  \u001b[0m | \u001b[95m398.8    \u001b[0m | \u001b[95m284.9    \u001b[0m | \u001b[95m0.8948   \u001b[0m | \u001b[95m0.5979   \u001b[0m | \u001b[95m0.9844   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003567 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003691 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m0.712    \u001b[0m | \u001b[0m0.8177   \u001b[0m | \u001b[0m0.0204   \u001b[0m | \u001b[0m70.35    \u001b[0m | \u001b[0m131.3    \u001b[0m | \u001b[0m0.3887   \u001b[0m | \u001b[0m0.2713   \u001b[0m | \u001b[0m0.9657   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003622 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011968 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m0.8344   \u001b[0m | \u001b[0m0.8714   \u001b[0m | \u001b[0m0.02881  \u001b[0m | \u001b[0m294.2    \u001b[0m | \u001b[0m85.23    \u001b[0m | \u001b[0m0.8022   \u001b[0m | \u001b[0m0.07455  \u001b[0m | \u001b[0m0.9974   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003656 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112025 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m0.8572   \u001b[0m | \u001b[0m0.9855   \u001b[0m | \u001b[0m0.06259  \u001b[0m | \u001b[0m380.6    \u001b[0m | \u001b[0m198.4    \u001b[0m | \u001b[0m0.5065   \u001b[0m | \u001b[0m0.8808   \u001b[0m | \u001b[0m0.9476   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003680 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003670 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m0.4369   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m395.9    \u001b[0m | \u001b[0m243.6    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003523 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080148 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m0.8548   \u001b[0m | \u001b[0m0.8486   \u001b[0m | \u001b[0m0.05808  \u001b[0m | \u001b[0m359.6    \u001b[0m | \u001b[0m180.2    \u001b[0m | \u001b[0m0.4202   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.8058   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003548 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012643 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[95m14       \u001b[0m | \u001b[95m0.8573   \u001b[0m | \u001b[95m0.8494   \u001b[0m | \u001b[95m0.09408  \u001b[0m | \u001b[95m422.4    \u001b[0m | \u001b[95m299.1    \u001b[0m | \u001b[95m0.1864   \u001b[0m | \u001b[95m0.9478   \u001b[0m | \u001b[95m0.9497   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090947 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011776 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[95m15       \u001b[0m | \u001b[95m0.8575   \u001b[0m | \u001b[95m0.8      \u001b[0m | \u001b[95m0.1      \u001b[0m | \u001b[95m373.6    \u001b[0m | \u001b[95m300.0    \u001b[0m | \u001b[95m0.1577   \u001b[0m | \u001b[95m0.2608   \u001b[0m | \u001b[95m0.9466   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003768 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011724 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m16       \u001b[0m | \u001b[0m0.8522   \u001b[0m | \u001b[0m0.9408   \u001b[0m | \u001b[0m0.06465  \u001b[0m | \u001b[0m133.5    \u001b[0m | \u001b[0m198.4    \u001b[0m | \u001b[0m0.8354   \u001b[0m | \u001b[0m0.5736   \u001b[0m | \u001b[0m0.951    \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011629 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012714 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m17       \u001b[0m | \u001b[0m0.8481   \u001b[0m | \u001b[0m0.91     \u001b[0m | \u001b[0m0.04142  \u001b[0m | \u001b[0m136.7    \u001b[0m | \u001b[0m238.4    \u001b[0m | \u001b[0m0.4656   \u001b[0m | \u001b[0m0.5622   \u001b[0m | \u001b[0m0.9696   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003709 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003839 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[95m18       \u001b[0m | \u001b[95m0.8578   \u001b[0m | \u001b[95m0.9544   \u001b[0m | \u001b[95m0.09999  \u001b[0m | \u001b[95m175.7    \u001b[0m | \u001b[95m257.6    \u001b[0m | \u001b[95m0.3682   \u001b[0m | \u001b[95m0.6902   \u001b[0m | \u001b[95m0.9797   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011888 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094972 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m19       \u001b[0m | \u001b[0m0.8478   \u001b[0m | \u001b[0m0.9277   \u001b[0m | \u001b[0m0.02665  \u001b[0m | \u001b[0m207.4    \u001b[0m | \u001b[0m231.7    \u001b[0m | \u001b[0m0.6528   \u001b[0m | \u001b[0m0.403    \u001b[0m | \u001b[0m0.9855   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003655 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102725 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m20       \u001b[0m | \u001b[0m0.8471   \u001b[0m | \u001b[0m0.9734   \u001b[0m | \u001b[0m0.02086  \u001b[0m | \u001b[0m211.8    \u001b[0m | \u001b[0m276.2    \u001b[0m | \u001b[0m0.648    \u001b[0m | \u001b[0m0.5825   \u001b[0m | \u001b[0m0.8338   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003637 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011713 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m21       \u001b[0m | \u001b[0m0.8564   \u001b[0m | \u001b[0m0.808    \u001b[0m | \u001b[0m0.09527  \u001b[0m | \u001b[0m171.2    \u001b[0m | \u001b[0m298.4    \u001b[0m | \u001b[0m0.608    \u001b[0m | \u001b[0m0.9204   \u001b[0m | \u001b[0m0.8385   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011963 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003558 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m22       \u001b[0m | \u001b[0m0.8561   \u001b[0m | \u001b[0m0.8129   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m135.0    \u001b[0m | \u001b[0m282.2    \u001b[0m | \u001b[0m0.2461   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.8      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003696 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003668 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m23       \u001b[0m | \u001b[0m0.844    \u001b[0m | \u001b[0m0.9484   \u001b[0m | \u001b[0m0.05976  \u001b[0m | \u001b[0m251.2    \u001b[0m | \u001b[0m78.02    \u001b[0m | \u001b[0m0.2612   \u001b[0m | \u001b[0m0.02317  \u001b[0m | \u001b[0m0.8977   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003733 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099227 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m24       \u001b[0m | \u001b[0m0.8532   \u001b[0m | \u001b[0m0.9862   \u001b[0m | \u001b[0m0.08424  \u001b[0m | \u001b[0m247.6    \u001b[0m | \u001b[0m126.7    \u001b[0m | \u001b[0m0.4601   \u001b[0m | \u001b[0m0.5347   \u001b[0m | \u001b[0m0.8169   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003694 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003706 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m25       \u001b[0m | \u001b[0m0.8502   \u001b[0m | \u001b[0m0.9301   \u001b[0m | \u001b[0m0.08637  \u001b[0m | \u001b[0m207.4    \u001b[0m | \u001b[0m114.2    \u001b[0m | \u001b[0m0.8213   \u001b[0m | \u001b[0m0.4957   \u001b[0m | \u001b[0m0.8354   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081601 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003535 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m26       \u001b[0m | \u001b[0m0.8536   \u001b[0m | \u001b[0m0.8483   \u001b[0m | \u001b[0m0.07042  \u001b[0m | \u001b[0m261.5    \u001b[0m | \u001b[0m172.5    \u001b[0m | \u001b[0m0.5763   \u001b[0m | \u001b[0m0.5448   \u001b[0m | \u001b[0m0.9837   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003685 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003821 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m27       \u001b[0m | \u001b[0m0.313    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m250.6    \u001b[0m | \u001b[0m214.1    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.8      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097987 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013016 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m28       \u001b[0m | \u001b[0m0.8542   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m281.9    \u001b[0m | \u001b[0m146.7    \u001b[0m | \u001b[0m0.2951   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003538 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011412 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m29       \u001b[0m | \u001b[0m0.8502   \u001b[0m | \u001b[0m0.8588   \u001b[0m | \u001b[0m0.09518  \u001b[0m | \u001b[0m169.9    \u001b[0m | \u001b[0m135.8    \u001b[0m | \u001b[0m0.05435  \u001b[0m | \u001b[0m0.8846   \u001b[0m | \u001b[0m0.9158   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003714 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003676 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m30       \u001b[0m | \u001b[0m0.2101   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m168.7    \u001b[0m | \u001b[0m90.92    \u001b[0m | \u001b[0m0.6592   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.8      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077287 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003575 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m31       \u001b[0m | \u001b[0m0.8518   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m147.8    \u001b[0m | \u001b[0m162.5    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003697 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003693 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m32       \u001b[0m | \u001b[0m0.8523   \u001b[0m | \u001b[0m0.9331   \u001b[0m | \u001b[0m0.06402  \u001b[0m | \u001b[0m91.61    \u001b[0m | \u001b[0m269.8    \u001b[0m | \u001b[0m0.02674  \u001b[0m | \u001b[0m0.1384   \u001b[0m | \u001b[0m0.9216   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101749 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003532 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m33       \u001b[0m | \u001b[0m0.8337   \u001b[0m | \u001b[0m0.8862   \u001b[0m | \u001b[0m0.01083  \u001b[0m | \u001b[0m393.0    \u001b[0m | \u001b[0m148.8    \u001b[0m | \u001b[0m0.591    \u001b[0m | \u001b[0m0.5844   \u001b[0m | \u001b[0m0.9195   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003704 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023384 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m34       \u001b[0m | \u001b[0m0.8258   \u001b[0m | \u001b[0m0.9396   \u001b[0m | \u001b[0m0.04681  \u001b[0m | \u001b[0m51.3     \u001b[0m | \u001b[0m245.6    \u001b[0m | \u001b[0m0.1625   \u001b[0m | \u001b[0m0.5789   \u001b[0m | \u001b[0m0.8545   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012054 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011445 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m35       \u001b[0m | \u001b[0m0.8464   \u001b[0m | \u001b[0m0.8812   \u001b[0m | \u001b[0m0.08482  \u001b[0m | \u001b[0m55.37    \u001b[0m | \u001b[0m299.5    \u001b[0m | \u001b[0m0.3061   \u001b[0m | \u001b[0m0.8737   \u001b[0m | \u001b[0m0.9697   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003683 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088879 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m36       \u001b[0m | \u001b[0m0.851    \u001b[0m | \u001b[0m0.9742   \u001b[0m | \u001b[0m0.06091  \u001b[0m | \u001b[0m404.0    \u001b[0m | \u001b[0m88.89    \u001b[0m | \u001b[0m0.4382   \u001b[0m | \u001b[0m0.1922   \u001b[0m | \u001b[0m0.8769   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003568 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003527 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m37       \u001b[0m | \u001b[0m0.7699   \u001b[0m | \u001b[0m0.8292   \u001b[0m | \u001b[0m0.007985 \u001b[0m | \u001b[0m415.2    \u001b[0m | \u001b[0m50.0     \u001b[0m | \u001b[0m0.362    \u001b[0m | \u001b[0m0.705    \u001b[0m | \u001b[0m0.9187   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003661 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003735 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m38       \u001b[0m | \u001b[0m0.8559   \u001b[0m | \u001b[0m0.9429   \u001b[0m | \u001b[0m0.08784  \u001b[0m | \u001b[0m434.3    \u001b[0m | \u001b[0m120.4    \u001b[0m | \u001b[0m0.6723   \u001b[0m | \u001b[0m0.588    \u001b[0m | \u001b[0m0.819    \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015597 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003525 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m39       \u001b[0m | \u001b[0m0.8566   \u001b[0m | \u001b[0m0.8898   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m440.3    \u001b[0m | \u001b[0m162.6    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.8994   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014305 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003697 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m40       \u001b[0m | \u001b[0m0.8563   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.0793   \u001b[0m | \u001b[0m474.8    \u001b[0m | \u001b[0m137.3    \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m0.9971   \u001b[0m | \u001b[0m0.971    \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014023 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003660 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m41       \u001b[0m | \u001b[0m0.8548   \u001b[0m | \u001b[0m0.9964   \u001b[0m | \u001b[0m0.03205  \u001b[0m | \u001b[0m480.9    \u001b[0m | \u001b[0m180.6    \u001b[0m | \u001b[0m0.9481   \u001b[0m | \u001b[0m0.7643   \u001b[0m | \u001b[0m0.8536   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003701 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003725 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m42       \u001b[0m | \u001b[0m0.8422   \u001b[0m | \u001b[0m0.942    \u001b[0m | \u001b[0m0.02101  \u001b[0m | \u001b[0m471.2    \u001b[0m | \u001b[0m94.89    \u001b[0m | \u001b[0m0.524    \u001b[0m | \u001b[0m0.4354   \u001b[0m | \u001b[0m0.9641   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003520 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115789 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m43       \u001b[0m | \u001b[0m0.8375   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.007507 \u001b[0m | \u001b[0m500.0    \u001b[0m | \u001b[0m222.0    \u001b[0m | \u001b[0m0.9191   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.8      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003517 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099141 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m44       \u001b[0m | \u001b[0m0.3145   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m315.9    \u001b[0m | \u001b[0m171.5    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.8      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003508 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096904 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m45       \u001b[0m | \u001b[0m0.8515   \u001b[0m | \u001b[0m0.817    \u001b[0m | \u001b[0m0.05696  \u001b[0m | \u001b[0m105.3    \u001b[0m | \u001b[0m299.9    \u001b[0m | \u001b[0m0.7451   \u001b[0m | \u001b[0m0.2538   \u001b[0m | \u001b[0m0.9132   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003686 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003702 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m46       \u001b[0m | \u001b[0m0.843    \u001b[0m | \u001b[0m0.9853   \u001b[0m | \u001b[0m0.0919   \u001b[0m | \u001b[0m284.4    \u001b[0m | \u001b[0m50.17    \u001b[0m | \u001b[0m0.1778   \u001b[0m | \u001b[0m0.4188   \u001b[0m | \u001b[0m0.8068   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095806 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011424 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m47       \u001b[0m | \u001b[0m0.8569   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m469.9    \u001b[0m | \u001b[0m300.0    \u001b[0m | \u001b[0m0.05908  \u001b[0m | \u001b[0m0.9092   \u001b[0m | \u001b[0m0.9992   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003576 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113295 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[95m48       \u001b[0m | \u001b[95m0.8579   \u001b[0m | \u001b[95m0.8812   \u001b[0m | \u001b[95m0.06148  \u001b[0m | \u001b[95m498.6    \u001b[0m | \u001b[95m271.0    \u001b[0m | \u001b[95m0.1691   \u001b[0m | \u001b[95m0.5366   \u001b[0m | \u001b[95m0.8401   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111320 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013325 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m49       \u001b[0m | \u001b[0m0.8571   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m464.5    \u001b[0m | \u001b[0m259.2    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.8      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003521 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003538 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m50       \u001b[0m | \u001b[0m0.8571   \u001b[0m | \u001b[0m0.8176   \u001b[0m | \u001b[0m0.05385  \u001b[0m | \u001b[0m457.8    \u001b[0m | \u001b[0m209.1    \u001b[0m | \u001b[0m0.7333   \u001b[0m | \u001b[0m0.6304   \u001b[0m | \u001b[0m0.8078   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017495 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011977 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m51       \u001b[0m | \u001b[0m0.8577   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m325.3    \u001b[0m | \u001b[0m300.0    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011665 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011869 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m52       \u001b[0m | \u001b[0m0.8571   \u001b[0m | \u001b[0m0.8026   \u001b[0m | \u001b[0m0.05252  \u001b[0m | \u001b[0m277.9    \u001b[0m | \u001b[0m299.6    \u001b[0m | \u001b[0m0.1108   \u001b[0m | \u001b[0m0.006479 \u001b[0m | \u001b[0m0.8122   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015912 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003668 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m53       \u001b[0m | \u001b[0m0.8348   \u001b[0m | \u001b[0m0.9922   \u001b[0m | \u001b[0m0.03034  \u001b[0m | \u001b[0m498.9    \u001b[0m | \u001b[0m54.92    \u001b[0m | \u001b[0m0.08165  \u001b[0m | \u001b[0m0.6028   \u001b[0m | \u001b[0m0.9589   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003639 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m54       \u001b[0m | \u001b[0m0.857    \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m240.7    \u001b[0m | \u001b[0m300.0    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003550 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003534 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m55       \u001b[0m | \u001b[0m0.05299  \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m50.0     \u001b[0m | \u001b[0m50.0     \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003691 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003764 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m56       \u001b[0m | \u001b[0m0.07468  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m50.0     \u001b[0m | \u001b[0m189.8    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.8      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003581 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012570 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m57       \u001b[0m | \u001b[0m0.8495   \u001b[0m | \u001b[0m0.8346   \u001b[0m | \u001b[0m0.03539  \u001b[0m | \u001b[0m499.7    \u001b[0m | \u001b[0m116.1    \u001b[0m | \u001b[0m0.6646   \u001b[0m | \u001b[0m0.9895   \u001b[0m | \u001b[0m0.9021   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003553 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011719 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m58       \u001b[0m | \u001b[0m0.8572   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m322.3    \u001b[0m | \u001b[0m259.7    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113541 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003738 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m59       \u001b[0m | \u001b[0m0.8251   \u001b[0m | \u001b[0m0.9551   \u001b[0m | \u001b[0m0.03002  \u001b[0m | \u001b[0m326.9    \u001b[0m | \u001b[0m50.08    \u001b[0m | \u001b[0m0.3434   \u001b[0m | \u001b[0m0.6922   \u001b[0m | \u001b[0m0.8004   \u001b[0m |\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097104 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score -0.001562\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003533 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 164652, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 0.001562\n",
            "| \u001b[0m60       \u001b[0m | \u001b[0m0.8487   \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m460.2    \u001b[0m | \u001b[0m50.0     \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.8      \u001b[0m |\n",
            "=============================================================================================================\n",
            "Best Hyperparameters: {'colsample_bytree': 0.8811603956653644, 'learning_rate': 0.061479375935141176, 'n_estimators': 498, 'num_leaves': 270, 'reg_alpha': 0.16911205087857273, 'reg_lambda': 0.5365830151209172, 'subsample': 0.8401403792708538}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-10-03T14:30:30.531562Z",
          "iopub.status.busy": "2023-10-03T14:30:30.531196Z",
          "iopub.status.idle": "2023-10-03T14:53:34.219508Z",
          "shell.execute_reply": "2023-10-03T14:53:34.218379Z",
          "shell.execute_reply.started": "2023-10-03T14:30:30.531533Z"
        },
        "id": "z3f7ONYQ1MZ0",
        "outputId": "d11fe8e0-df18-46b4-b7fc-39223918f3f5",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start fit....\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008057 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296373, number of used features: 7\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Info] Start training from score -0.000389\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[498]\ttraining's l2: 0.107065\tvalid_1's l2: 0.133756\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007612 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296373, number of used features: 7\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Info] Start training from score 0.000891\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[495]\ttraining's l2: 0.107315\tvalid_1's l2: 0.131101\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007703 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296373, number of used features: 7\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Info] Start training from score -0.000059\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[497]\ttraining's l2: 0.106873\tvalid_1's l2: 0.13337\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007664 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296373, number of used features: 7\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Info] Start training from score -0.000944\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[498]\ttraining's l2: 0.106276\tvalid_1's l2: 0.136446\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007686 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296374, number of used features: 7\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Info] Start training from score -0.000363\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[498]\ttraining's l2: 0.106562\tvalid_1's l2: 0.138181\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033615 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296374, number of used features: 7\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Info] Start training from score 0.001121\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[494]\ttraining's l2: 0.106937\tvalid_1's l2: 0.133518\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008042 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296374, number of used features: 7\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Info] Start training from score -0.000311\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[497]\ttraining's l2: 0.106865\tvalid_1's l2: 0.138303\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007640 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296374, number of used features: 7\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Info] Start training from score 0.000408\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[498]\ttraining's l2: 0.106835\tvalid_1's l2: 0.134205\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033700 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296374, number of used features: 7\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Info] Start training from score 0.000071\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[497]\ttraining's l2: 0.106471\tvalid_1's l2: 0.133705\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007645 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 296374, number of used features: 7\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Info] Start training from score -0.000424\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[498]\ttraining's l2: 0.10602\tvalid_1's l2: 0.1446\n",
            "mean_train_RMSE:0.326682777431503\n",
            "mean_valid_RMSE:0.3683669889171216\n"
          ]
        }
      ],
      "source": [
        "print(\"start fit....\")\n",
        "folds = 10 #Divide the data into 10 parts\n",
        "y = main_df['power']\n",
        "X = main_df.drop(['power'],axis=1)\n",
        "\n",
        "train_RMSE=[]\n",
        "valid_RMSE=[]\n",
        "\n",
        "# Store the list of learned models\n",
        "models = []\n",
        "\n",
        "#Shuffle the data set randomly and divide it into folds\n",
        "kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
        "# Add the early_stopping_rounds parameter\n",
        "best_params['early_stopping_rounds'] = 100\n",
        "\n",
        "#Divide x_train into a training set and a verification set in a ratio of 9:1, and take out the subscripts\n",
        "for train_index, valid_index in kf.split(X):\n",
        "\n",
        "    #Get the data of the training set and validation set based on the subscripts\n",
        "    x_train_cv = X.iloc[train_index]\n",
        "    y_train_cv = y.iloc[train_index]\n",
        "    x_valid_cv =X.iloc[valid_index]\n",
        "    y_valid_cv = y.iloc[valid_index]\n",
        "\n",
        "    #Call the LightGBM regression model and add parameters\n",
        "    model = LGBMRegressor(**best_params)\n",
        "\n",
        "    #Use x_train_cv to train the model, and use x_train_cv and x_valid_cv to evaluate together\n",
        "    model.fit(\n",
        "        x_train_cv,\n",
        "        y_train_cv,\n",
        "        eval_set = [(x_train_cv, y_train_cv), (x_valid_cv, y_valid_cv)],\n",
        "        #verbose = 100, #Iterate 100 times and output a result\n",
        "    )\n",
        "\n",
        "    #Predict the training set\n",
        "    y_pred_train = model.predict(x_train_cv, num_iteration=model.best_iteration_)\n",
        "    #Predict on the validation set\n",
        "    y_pred_valid = model.predict(x_valid_cv, num_iteration=model.best_iteration_)\n",
        "\n",
        "    train_rmse=RMSE(y_pred_train,y_train_cv)\n",
        "    valid_rmse=RMSE(y_pred_valid,y_valid_cv)\n",
        "\n",
        "    train_RMSE.append(train_rmse)\n",
        "    valid_RMSE.append(valid_rmse)\n",
        "    #Save model into list\n",
        "    models.append(model)\n",
        "    #print(f\"train_RMSE:{train_RMSE},valid_RMSE:{valid_RMSE}\")\n",
        "\n",
        "train_RMSE=np.array(train_RMSE)\n",
        "valid_RMSE=np.array(valid_RMSE)\n",
        "\n",
        "print(f\"mean_train_RMSE:{np.mean(train_RMSE)}\")\n",
        "print(f\"mean_valid_RMSE:{np.mean(valid_RMSE)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-03T14:53:34.223277Z",
          "iopub.status.busy": "2023-10-03T14:53:34.220831Z",
          "iopub.status.idle": "2023-10-03T14:53:42.156365Z",
          "shell.execute_reply": "2023-10-03T14:53:42.155235Z",
          "shell.execute_reply.started": "2023-10-03T14:53:34.223230Z"
        },
        "id": "BgFmECi01MZ1",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51c939e-bcf7-4236-f94b-120d8bf24f61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0.        ,   0.        ,   0.        ,   0.        ,\n",
              "         2.73785254,   0.        ,   0.        ,   4.09014963,\n",
              "         0.        ,   0.        ,   0.        ,   0.        ,\n",
              "         0.        ,   0.        ,   0.        ,   0.        ,\n",
              "        38.50192914, 216.41721741, 351.44964111, 680.91127683,\n",
              "       552.11326185, 232.4273943 ,  22.36853196,   0.        ,\n",
              "         0.        ,   0.        ,   0.        ,   0.        ,\n",
              "         3.60043492,   0.        ,   0.        ,   2.549007  ,\n",
              "         0.        ,   0.        ,   0.        ,   0.        ,\n",
              "         0.        ,   0.        ,   0.        ,   0.        ,\n",
              "        37.46956574, 217.66641729, 366.95804974, 713.29926661,\n",
              "       580.37642625, 253.34916334,  28.75712768,   1.69982158,\n",
              "         0.        ,   0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "test_X = prediction_df.drop(['power'],axis=1).values\n",
        "#Use each saved model to predict x_test once, and then take the average\n",
        "preds_test = []\n",
        "\n",
        "for model in models:\n",
        "\n",
        "    pred = model.predict(test_X, num_iteration=model.best_iteration_)\n",
        "\n",
        "    preds_test.append(pred)\n",
        "\n",
        "# Reverse the normalization\n",
        "original_predictions = np.array(preds_test) * main_df_power_std + main_df_power_mean\n",
        "#Convert the prediction results into np.array\n",
        "preds_test_np = np.array(original_predictions)\n",
        "#Average the prediction results of each model by column\n",
        "test_pred= preds_test_np.mean(axis=0)\n",
        "test_pred=np.where(test_pred<=0,0,test_pred)\n",
        "test_pred[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-03T14:53:42.158252Z",
          "iopub.status.busy": "2023-10-03T14:53:42.157836Z",
          "iopub.status.idle": "2023-10-03T14:53:42.230039Z",
          "shell.execute_reply": "2023-10-03T14:53:42.228945Z",
          "shell.execute_reply.started": "2023-10-03T14:53:42.158214Z"
        },
        "id": "1-Art-s01MZ1",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "f8400994-2716-47f5-dd3c-3ff66d6b4638"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id     power\n",
              "0  8401  0.000000\n",
              "1  8402  0.000000\n",
              "2  8403  0.000000\n",
              "3  8404  0.000000\n",
              "4  8405  2.737853"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21ddc3bd-8279-49d8-8166-9c336273c91e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>power</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8401</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8402</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8403</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8404</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8405</td>\n",
              "      <td>2.737853</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21ddc3bd-8279-49d8-8166-9c336273c91e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-21ddc3bd-8279-49d8-8166-9c336273c91e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-21ddc3bd-8279-49d8-8166-9c336273c91e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-96d7a816-c163-4f27-8a83-49fc085ed571\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-96d7a816-c163-4f27-8a83-49fc085ed571')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-96d7a816-c163-4f27-8a83-49fc085ed571 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "submission=pd.read_csv(\"sample_submission.csv\")\n",
        "submission['power']=test_pred\n",
        "submission.to_csv(\"baseline.csv\",index=None)\n",
        "submission.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network model\n",
        "Using a Multi Layer model for predection"
      ],
      "metadata": {
        "id": "J_KLcDGBqXG1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "66Ot1pELDl8v",
        "outputId": "c0ca0f37-89d9-4079-e00e-3a41eb67dcbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b8674a8c0f0>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Test split"
      ],
      "metadata": {
        "id": "GUK4_QKNQsgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_test_ratio = 0.8\n",
        "train_dataset = main_df[ : int(len(train_df)*train_test_ratio)]\n",
        "test_dataset = main_df[int(len(train_df)*train_test_ratio) : ]"
      ],
      "metadata": {
        "id": "5Xn7NQgrQxS4"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "5wEwxZLgHwxR"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.Tensor(train_dataset.drop(['power'],axis=1).values.astype(np.float32))\n",
        "y_train_tensor = torch.Tensor(train_dataset['power'].values.astype(np.float32))\n",
        "\n",
        "X_test_tensor = torch.Tensor(test_dataset.drop(['power'],axis=1).values.astype(np.float32))\n",
        "y_test_tensor = torch.Tensor(test_dataset['power'].values.astype(np.float32))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor.shape , y_train_tensor.shape"
      ],
      "metadata": {
        "id": "vYSBua84R2De",
        "outputId": "b6930a35-1f21-4e74-830e-15883db88793",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([263443, 7]), torch.Size([263443]))"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "hbDd2kcyJTGU"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "id": "UqKqJVwB16fE",
        "outputId": "8ed9f301-df16-4167-a79d-0780b7d62ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/100.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.3.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-23.9.7-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.2.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-23.9.7 scikit-optimize-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.datasets import make_regression\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define your PyTorch Neural Network Model for regression\n",
        "class MyNNRegression(nn.Module):\n",
        "    def __init__(self, input_size, hidden_units, num_layers):\n",
        "        super(MyNNRegression, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(input_size, hidden_units))\n",
        "        self.layers.append(nn.ReLU())\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.layers.append(nn.Linear(hidden_units, hidden_units))\n",
        "            self.layers.append(nn.ReLU())\n",
        "        self.layers.append(nn.Linear(hidden_units, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Define the objective function for regression\n",
        "def objective(num_layers, learning_rate, hidden_units):\n",
        "    num_layers = int(num_layers)\n",
        "    hidden_units = int(hidden_units)\n",
        "\n",
        "    # Build and train the PyTorch model with the suggested hyperparameters\n",
        "    model = MyNNRegression(input_size=X_train_tensor.shape[1], hidden_units=hidden_units, num_layers=num_layers)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    epochs = 10\n",
        "    # Training loop (replace this with your own training loop)\n",
        "    for batch, (inputs, targets) in enumerate(train_loader):\n",
        "      targets = targets.unsqueeze(dim=1)\n",
        "      # 1. Forward pass\n",
        "      outputs = nn_model(inputs)\n",
        "      # 2. Calculate loss (per batch)\n",
        "      loss = torch.sqrt(criterion(outputs, targets))\n",
        "      train_loss += loss # accumulatively add up the loss per epoch\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "      # 4. Loss backward\n",
        "      loss.backward()\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "\n",
        "    # Setup variables for accumulatively adding up loss and accuracy\n",
        "    test_loss = 0\n",
        "    nn_model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for inputs, targets in test_loader:\n",
        "        targets = targets.unsqueeze(dim=1)\n",
        "        # 1. Forward pass\n",
        "        test_outputs = nn_model(inputs)\n",
        "        # 2. Calculate loss (accumatively)\n",
        "        test_loss += torch.sqrt(criterion(test_outputs, targets))\n",
        "\n",
        "      # Divide total test loss by length of test dataloader (per epoch)\n",
        "      test_loss /= len(test_loader)\n",
        "\n",
        "    return test_loss\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "param_space = {\n",
        "    'num_layers': (1, 5),          # Range for the number of layers\n",
        "    'learning_rate': (1e-5, 1e-1, 'log-uniform'),  # Range for the learning rate\n",
        "    'hidden_units': (8,16),     # Range for the number of hidden units\n",
        "}\n",
        "\n",
        "base_estimator = MyNNRegression(input_size=7, hidden_units=16, num_layers=2)\n",
        "optimizer = BayesSearchCV(\n",
        "    base_estimator,\n",
        "    param_space,\n",
        "    n_iter=50,   # Number of optimization steps\n",
        "    random_state=42,\n",
        "    scoring='neg_mean_squared_error',  # Use a regression metric\n",
        ")\n",
        "\n",
        "# Run the optimization\n",
        "optimizer.fit(None)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = optimizer.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n"
      ],
      "metadata": {
        "id": "id_fIQQC1rs8",
        "outputId": "a7a70c65-187f-4e80-82be-e66d56d05285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-4341eb93ab0d>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# Run the optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Get the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skopt/searchcv.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_kwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# BaseSearchCV never ranked train scores,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_multimetric_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \"\"\"\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;34m\"estimator should be an estimator implementing 'fit' method, %r was passed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: estimator should be an estimator implementing 'fit' method, MyNNRegression(\n  (layers): ModuleList(\n    (0): Linear(in_features=7, out_features=16, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=16, out_features=16, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=16, out_features=1, bias=True)\n  )\n) was passed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgKSPZGYJck3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiJHj_nZJhrs"
      },
      "outputs": [],
      "source": [
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, input_features, hidden_units, output_size):\n",
        "        super().__init__()\n",
        "        self.linear_layer_stack = nn.Sequential(\n",
        "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units, out_features=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=4, out_features=output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layer_stack(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWtI_sbiKb-o"
      },
      "outputs": [],
      "source": [
        "nn_model = RegressionModel(input_features= X_train_tensor.shape[1],\n",
        "                    output_size=1,\n",
        "                    hidden_units= 8).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AQS2hVqLdTo"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
        "optimizer = optim.AdamW(nn_model.parameters(), lr=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jR8o-42S5cI",
        "outputId": "04eed6f2-5f90-4ee5-9bc2-ce9e50d3e0ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output shape =  torch.Size([128, 1])\n",
            "targets shape =  torch.Size([128, 1])\n",
            "loss value =  1.1553386449813843\n"
          ]
        }
      ],
      "source": [
        "## try a single run on the model to check the output shapes\n",
        "nn_model.train()\n",
        "for inputs, targets in train_loader:\n",
        "    output = nn_model(inputs)\n",
        "    print(\"output shape = \", output.shape)\n",
        "    targets = targets.unsqueeze(dim=1)\n",
        "    print(\"targets shape = \", targets.shape)\n",
        "    loss = criterion(output, targets)\n",
        "    print(\"loss value = \", loss.item())\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xkkD3aYMCFC",
        "outputId": "22de5250-f9f0-4755-b0aa-9cab699d4d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "-------\n",
            "\n",
            "Train loss: 0.79628 | Test loss: 0.98649\n",
            "Epoch: 1\n",
            "-------\n",
            "\n",
            "Train loss: 0.75656 | Test loss: 1.03925\n",
            "Epoch: 2\n",
            "-------\n",
            "\n",
            "Train loss: 0.74396 | Test loss: 1.05232\n",
            "Epoch: 3\n",
            "-------\n",
            "\n",
            "Train loss: 0.73867 | Test loss: 1.13611\n",
            "Epoch: 4\n",
            "-------\n",
            "\n",
            "Train loss: 0.73675 | Test loss: 1.07861\n",
            "Epoch: 5\n",
            "-------\n",
            "\n",
            "Train loss: 0.73654 | Test loss: 1.07169\n",
            "Epoch: 6\n",
            "-------\n",
            "\n",
            "Train loss: 0.73681 | Test loss: 1.16946\n",
            "Epoch: 7\n",
            "-------\n",
            "\n",
            "Train loss: 0.73753 | Test loss: 1.05498\n",
            "Epoch: 8\n",
            "-------\n",
            "\n",
            "Train loss: 0.73740 | Test loss: 1.14926\n",
            "Epoch: 9\n",
            "-------\n",
            "\n",
            "Train loss: 0.73784 | Test loss: 1.04795\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Epoch: {epoch}\\n-------\")\n",
        "\n",
        "  nn_model.train()\n",
        "  ### Training\n",
        "  train_loss = 0\n",
        "  for batch, (inputs, targets) in enumerate(train_loader):\n",
        "    targets = targets.unsqueeze(dim=1)\n",
        "    # 1. Forward pass\n",
        "    outputs = nn_model(inputs)\n",
        "    # 2. Calculate loss (per batch)\n",
        "    loss = torch.sqrt(criterion(outputs, targets))\n",
        "    train_loss += loss # accumulatively add up the loss per epoch\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "    #if batch % 400 == 0:\n",
        "      #print(f\"Looked at {batch * len(inputs)}/{len(train_loader.dataset)} samples\")\n",
        "\n",
        "  # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
        "  train_loss /= len(train_loader)\n",
        "  ### Testing\n",
        "  # Setup variables for accumulatively adding up loss and accuracy\n",
        "  test_loss = 0\n",
        "  nn_model.eval()\n",
        "  with torch.inference_mode():\n",
        "    rmse_sum = 0.0\n",
        "    num_samples = 0\n",
        "    for inputs, targets in test_loader:\n",
        "      targets = targets.unsqueeze(dim=1)\n",
        "      # 1. Forward pass\n",
        "      test_outputs = nn_model(inputs)\n",
        "      # 2. Calculate loss (accumatively)\n",
        "      test_loss += torch.sqrt(criterion(test_outputs, targets))\n",
        "\n",
        "    # Divide total test loss by length of test dataloader (per epoch)\n",
        "    test_loss /= len(test_loader)\n",
        "  ## Print out what's happening\n",
        "  print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}